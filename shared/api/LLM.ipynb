{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai"
      ],
      "metadata": {
        "id": "Ek9qChhv_Bt9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "pYPeww68_DoV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "print(\"Available models:\")\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "PhjvMwntCpeW",
        "outputId": "63652e68-065f-4934-c74b-6300c0c75ab0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models:\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "orh8Y5yE-8-J",
        "outputId": "734ec324-b366-41f2-d868-7c2fa10ddea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending prompt to the model...\n",
            "------------------------------\n",
            "Prompt:\n",
            "\n",
            "\n",
            "You are an AI assistant that translates natural language commands into a step-by-step plan for interacting with a web user interface.\n",
            "Assume you are already logged into the target website.\n",
            "Your task is to break down the user's request into a sequence of simple, clear actions.\n",
            "\n",
            "\n",
            "**User Request:**\n",
            "هل يمكنك من فضلك تنزيل آخر محاضرة لأمن الشبكات من مودل الجامعة الدولية؟\n",
            "\n",
            "**Instruction:**\n",
            "\n",
            "Based on the user's request, generate a numbered list of actions to be performed on the user interface.\n",
            "Identify the key entities (like course name, action, platform).\n",
            "The final step should explicitly mention the goal. Let's think step by step.\n",
            "\n",
            "\n",
            "\n",
            "------------------------------\n",
            "LLM's Reasoning and Answer:\n",
            "------------------------------\n",
            "بالتأكيد، إليك خطة مفصلة خطوة بخطوة لتنفيذ طلبك.\n",
            "\n",
            "**الكيانات الأساسية:**\n",
            "*   **المنصة:** مودل الجامعة الدولية (International University's Moodle)\n",
            "*   **اسم المقرر:** أمن الشبكات (Network Security)\n",
            "*   **الإجراء:** تنزيل (Download)\n",
            "*   **العنصر المطلوب:** آخر محاضرة (The latest lecture)\n",
            "\n",
            "**الخطوات:**\n",
            "\n",
            "1.  اذهب إلى صفحة \"مقرراتي الدراسية\" أو لوحة التحكم الرئيسية في نظام مودل.\n",
            "2.  ابحث عن مقرر **\"أمن الشبكات\"** ضمن قائمة المقررات وانقر عليه.\n",
            "3.  داخل صفحة المقرر، تصفح المحتوى للوصول إلى القسم الذي يحتوي على أحدث محاضرة (قد تكون في الأعلى أو في الأسفل حسب ترتيب المواضيع).\n",
            "4.  حدد موقع رابط ملف المحاضرة (على سبيل المثال، ملف PDF أو PowerPoint).\n",
            "5.  انقر على رابط الملف لبدء عملية التنزيل.\n",
            "6.  **الهدف النهائي:** تم تنزيل ملف آخر محاضرة لمادة أمن الشبكات بنجاح على جهازك.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Get the API key from Colab's secret manager\n",
        "    GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Could not configure the API key. Please ensure you have set it correctly in the 'Secrets' tab. Error: {e}\")\n",
        "    # Exit if the key isn't found to avoid further errors.\n",
        "    exit()\n",
        "\n",
        "# 3. Initialize the model\n",
        "# Using gemini-1.5-flash as it's fast and capable for this task.\n",
        "# Use the full, correct model name\n",
        "# Use the stable and widely available 'gemini-pro' model\n",
        "model = genai.GenerativeModel('gemini-pro-latest')\n",
        "\n",
        "# The specific user command to be interpreted.\n",
        "user_request = \"هل يمكنك من فضلك تنزيل آخر محاضرة لأمن الشبكات من مودل الجامعة الدولية؟\"\n",
        "\n",
        "# --- NEW PROMPT COMPONENTS ---\n",
        "\n",
        "# 1. New Context: Define the LLM's role as a UI assistant.\n",
        "context = \"\"\"\n",
        "You are an AI assistant that translates natural language commands into a step-by-step plan for interacting with a web user interface.\n",
        "Assume you are already logged into the target website.\n",
        "Your task is to break down the user's request into a sequence of simple, clear actions.\n",
        "\"\"\"\n",
        "\n",
        "# 2. New Instruction: Guide the LLM on how to create the plan.\n",
        "instruction = \"\"\"\n",
        "Based on the user's request, generate a numbered list of actions to be performed on the user interface.\n",
        "Identify the key entities (like course name, action, platform).\n",
        "The final step should explicitly mention the goal. Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = f\"\"\"\n",
        "{context}\n",
        "\n",
        "**User Request:**\n",
        "{user_request}\n",
        "\n",
        "**Instruction:**\n",
        "{instruction}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"Sending prompt to the model...\")\n",
        "print(\"------------------------------\")\n",
        "print(f\"Prompt:\\n{prompt}\")\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(\"\\n------------------------------\")\n",
        "print(\"LLM's Reasoning and Answer:\")\n",
        "print(\"------------------------------\")\n",
        "print(response.text)"
      ]
    }
  ]
}